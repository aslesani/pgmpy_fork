{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aslesani/pgmpy_fork/blob/master/src/default_test/imdb_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OmJrgoGBTfgJ",
    "outputId": "39d708de-ab42-4288-bee6-7bcc5970b9b4"
   },
   "outputs": [],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "# Notes\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "from read_write import data_preparation_for_sequences_based_deep_models, convert_binary_classes_to_zero_and_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJSdFzlqcakL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_val_graph(history):\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(1, len(loss) + 1)\n",
    "  print('epochs:' , epochs)\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChXfRjq4jJko"
   },
   "outputs": [],
   "source": [
    "def get_max_len_of_sequences(list_of_sequences):\n",
    "  lengths = [len(list_of_sequences[i]) for i in range(len(list_of_sequences))]\n",
    "  return max(lengths) , min(lengths) , lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_zUyBhXljmq"
   },
   "outputs": [],
   "source": [
    "def get_set_of_sensor_events(list_of_sequences):\n",
    " \n",
    "  set_of_sensor_events = set()\n",
    "  \n",
    "  for i in range(len(list_of_sequences)):\n",
    "      set_of_sensor_events = set_of_sensor_events.union(set(list_of_sequences[i]))\n",
    "  \n",
    "  return set_of_sensor_events, len(set_of_sensor_events)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "PSnIdgbbWlm6",
    "outputId": "fafa8eb3-bf07-40db-c168-948bf988ded0"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/aslesani/pgmpy_fork.git\n",
    "#ls\n",
    "#!git clone https://github.com/aslesani/created_dataset.git\n",
    "#!rm -r pgmpy_fork  \n",
    "#cd pgmpy_fork/src/default_test\n",
    "#cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Adhup5wx35Il"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def mcor(y_true, y_pred):\n",
    "    #matthews_correlation\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    " \n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    " \n",
    " \n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    " \n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    " \n",
    " \n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w0em365aEiF1",
    "outputId": "c94c0dd6-1e9f-484d-e710-5a7a07f62720"
   },
   "outputs": [],
   "source": [
    "#!pip install tabulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "wRPxCWPzExrc",
    "outputId": "a2e3e2fd-0584-40cc-ac2e-ca74216c0867"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "#print(tabulate([['Alice', 24], ['Bob', 19]], headers=['algorithm', 'acc']))\n",
    "\n",
    "def print_list_of_lists(data , headers):\n",
    "    print(tabulate(data, headers=headers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_print_list_of_lists():\n",
    "    data = [['Alice', 24], ['Bob', 19]]\n",
    "    headers=['algorithm', 'acc']\n",
    "    print_list_of_lists(data , headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm      acc\n",
      "-----------  -----\n",
      "Alice           24\n",
      "Bob             19\n"
     ]
    }
   ],
   "source": [
    "test_print_list_of_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM2IkLSSM458"
   },
   "outputs": [],
   "source": [
    "def imdb_lstm_data_preparation(max_features = 20000, maxlen = 80):\n",
    "  #max_features = 20000#number_of_events\n",
    "  # cut texts after this number of words (among top max_features most common words)\n",
    "  #maxlen = 10#max_seq_len\n",
    "\n",
    "  print('Loading data...')\n",
    "  (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "  print(len(x_train), 'train sequences')\n",
    "  print(len(x_test), 'test sequences')\n",
    "\n",
    "  #print('before apply pad_sequence, x_train[0]:' , x_train[0])\n",
    "\n",
    "  print('Pad sequences (samples x time)')\n",
    "  x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "  x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "  print('x_train shape:', x_train.shape)\n",
    "  print('x_test shape:', x_test.shape)\n",
    "  \n",
    "  return x_train, x_test, y_train, y_test, max_features, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Npd2VghtklVO"
   },
   "outputs": [],
   "source": [
    "#! cd pgmpy_fork/src/default_test\n",
    "#!ls\n",
    "#!git clone https://github.com/pgmpy/pgmpy \n",
    "#cd ..\n",
    "#!ls\n",
    "#!cd pgmpy/\n",
    "#pip install -r requirements.txt\n",
    "#!python setup.py install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gFSdJslP4bJQ",
    "outputId": "b6eb0ce9-d3da-46c8-c486-5a20639a3cb8"
   },
   "outputs": [],
   "source": [
    "max_seq_len, min_seq_len , lens = get_max_len_of_sequences(sequences)\n",
    "print(max_seq_len, min_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EITeYNgVRqNq",
    "outputId": "40a31c91-ad62-4182-aab3-29651e83eae8"
   },
   "outputs": [],
   "source": [
    "print(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ladLQfIuRfdm"
   },
   "outputs": [],
   "source": [
    "x_train = x_train[0:2500]\n",
    "y_train = y_train[0:2500]\n",
    "x_test = x_test[2501:3200]\n",
    "y_test = y_test[2501:3200]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7Xg1Rd7SvYw"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ZaeKy5XuUss0",
    "outputId": "98be588a-f67e-486e-9e41-d7bda043b8e7"
   },
   "outputs": [],
   "source": [
    "print((y_train[0:10]))\n",
    "print((my_x_train[0:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_without_embedding(x_train, y_train,x_test, y_test, max_features,embedding_vector_dim = 64, batch_size = 32, epochs = 5, \n",
    "                                   loss = 'binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'], plot_train_val_graph = False):\n",
    "  \n",
    "  #batch_size = 32\n",
    "\n",
    "  print('Build model...')\n",
    "  model = Sequential()\n",
    "  #model.add(Embedding(max_features+1, embedding_vector_dim))\n",
    "  #model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "  #model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "  model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  # try using different optimizers and different optimizer configs\n",
    "  model.compile(loss= loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics= metrics)#, mcor,recall, f1])\n",
    "\n",
    "  print('Train...')\n",
    "  history = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test))\n",
    "  score, acc = model.evaluate(x_test, y_test,\n",
    "                              batch_size=batch_size)\n",
    "  print('Test score:', score)# i think score is loss value\n",
    "  print('Test accuracy:', acc)\n",
    " \n",
    "  if plot_train_val_graph:\n",
    "      plot_train_val_graph(history)\n",
    "      \n",
    "  return score, acc, history, len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsUaDYDeTx3t"
   },
   "outputs": [],
   "source": [
    "def create_model_and_apply_on_data(x_train, y_train,x_test, y_test, max_features,\n",
    "                                   embedding_vector_dim = 64, batch_size = 32, epochs = 5, \n",
    "                                   loss = 'binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'], \n",
    "                                   plot_train_val_graph = False,\n",
    "                                   number_of_lstm_layers = 1,\n",
    "                                   ID_of_layer_to_repeat = 0):\n",
    "  '''\n",
    "  Parameters:\n",
    "  ===============\n",
    "  number_of_lstm_layers (default value = 1)\n",
    "      indicate the number of layers in stack of layers\n",
    "  \n",
    "  ID_of_layer_to_repeat (default value = 0)\n",
    "     0: LSTM\n",
    "     1:RNN\n",
    "     2: GRU\n",
    "  \n",
    "  '''\n",
    "  #batch_size = 32\n",
    "\n",
    "  print('Build model...')\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features+1, embedding_vector_dim))\n",
    "  #model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "  #model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "\n",
    "  if ID_of_layer_to_repeat == 0:    \n",
    "      model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\n",
    "      model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "  elif ID_of_layer_to_repeat == 1:\n",
    "      model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "  elif ID_of_layer_to_repeat == 2:\n",
    "      model.add(GRU(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "     \n",
    "        \n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  # try using different optimizers and different optimizer configs\n",
    "  model.compile(loss= loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics= metrics)#, mcor,recall, f1])\n",
    "\n",
    "  print('Train...')\n",
    "  history = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test))\n",
    "  score, acc = model.evaluate(x_test, y_test,\n",
    "                              batch_size=batch_size)\n",
    "  print('Test score:', score)# i think score is loss value\n",
    "  print('Test accuracy:', acc)\n",
    " \n",
    "  print(model.layers[0].output)\n",
    "  if plot_train_val_graph:\n",
    "      plot_train_val_graph(history)\n",
    "      \n",
    "  return score, acc, history, len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMsynyAJJCnJ"
   },
   "outputs": [],
   "source": [
    "def select_hyperparameters(address_to_read, ID_of_layer_to_repeat):\n",
    "    #address_to_read= r\"E:/pgmpy/Seq of sensor events_based on activities/based_on_activities.csv\"\n",
    "    #address_to_read = r\"E:\\pgmpy\\Seq of sensor events_no overlap_based on different deltas\\delta_{}min.csv\"\n",
    "    #address_to_read = r\"E:\\pgmpy\\Seq of sensor events_based_on_activity_and_no_overlap_delta\\delta_{}min.csv\"\n",
    "    results = []\n",
    "    for delta in list(range(1,16)) + [30,45,60,75,90,100]:#, 120,150, 180,200,240,300,400,500,600,700,800,900,1000]: #:\n",
    "        print(\"delta:\" , delta)\n",
    "        x_train, x_test, y_train, y_test, max_features, maxlen = data_preparation_for_sequences_based_deep_models(address_to_read.format(delta))\n",
    "        test_score, test_acc, history, num_of_train_samples, num_of_test_sample = create_model_and_apply_on_data(x_train =x_train, \n",
    "                                                                                                y_train = y_train,\n",
    "                                                                                                x_test = x_test, \n",
    "                                                                                                y_test = y_test, \n",
    "                                                                                                max_features = max_features,\n",
    "                                                                                                ID_of_layer_to_repeat = ID_of_layer_to_repeat )\n",
    "        results.append([delta, \n",
    "                        num_of_train_samples, \n",
    "                        num_of_test_sample, \n",
    "                        np.mean(history.history['loss']), \n",
    "                        np.mean(history.history['acc']),\n",
    "                        history.history['acc'][-1] ,\n",
    "                        test_score, \n",
    "                        test_acc])#, history.history\n",
    "        #print(history.history)\n",
    "    print_list_of_lists(results, ['delta(min)' ,\n",
    "                                  '#train', \n",
    "                                  '#test', \n",
    "                                  'train loss ', \n",
    "                                  'train acc(mean)', \n",
    "                                  'train_acc(last)', \n",
    "                                  'val loss ', \n",
    "                                  'val acc', ])#'history'\n",
    "    best_val_acc_index = np.argmax(results[:][-1])\n",
    "    print(\"****************************************\")\n",
    "    print(\"best vlidation acc delta:\" , results[best_val_acc_index][0])\n",
    "    \n",
    "    return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\pgmpy\\Twor2009\\Seq of sensor events_based_on_activity_and_no_overlap_delta\\delta_{}min.csv\n",
      "ID_of_layer_to_repeat: 0\n",
      "delta: 1\n",
      "[7, 8, 7, 10, 11, 8, 11, 8]\n",
      "[7, 11, 8, 7, 10, 11, 8, 7, 8, 7, 10, 8, 8, 7, 33, 11, 11, 11, 8, 86]\n",
      "[85, 86, 85, 7, 10, 11, 11, 62, 8, 65, 7, 86, 8, 85, 7, 34, 8, 8, 8, 8, 7, 8, 7, 62, 61, 8, 65, 62, 57, 65]\n",
      "[66, 66, 29, 58, 9, 30, 31, 43, 41, 13, 32, 42, 46, 43, 41, 42, 46]\n",
      "[43, 41, 31, 46]\n",
      "[42, 41, 9, 32, 13, 42, 9, 31, 32, 13]\n",
      "[31, 32, 31, 32, 31, 32, 31, 32, 31, 32]\n",
      "[31, 32, 31, 32, 31, 32, 31, 9, 32]\n",
      "[9, 13, 9, 13, 9, 13, 9, 13, 9]\n",
      "[13, 9, 13, 9, 9, 13, 9, 13, 9, 13, 9]\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (13558, 20)\n",
      "x_test shape: (3389, 20)\n",
      "#####################\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  7  8  7 10 11  8 11  8]\n",
      "[ 7 11  8  7 10 11  8  7  8  7 10  8  8  7 33 11 11 11  8 86]\n",
      "[ 7 86  8 85  7 34  8  8  8  8  7  8  7 62 61  8 65 62 57 65]\n",
      "[ 0  0  0 66 66 29 58  9 30 31 43 41 13 32 42 46 43 41 42 46]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 43 41 31 46]\n",
      "[ 0  0  0  0  0  0  0  0  0  0 42 41  9 32 13 42  9 31 32 13]\n",
      "[ 0  0  0  0  0  0  0  0  0  0 31 32 31 32 31 32 31 32 31 32]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0 31 32 31 32 31 32 31  9 32]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  9 13  9 13  9 13  9 13  9]\n",
      "[ 0  0  0  0  0  0  0  0  0 13  9 13  9  9 13  9 13  9 13  9]\n",
      "Build model...\n",
      "WARNING:tensorflow:From c:\\python37_64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          7808      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 73,921\n",
      "Trainable params: 73,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "WARNING:tensorflow:From c:\\python37_64\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 13558 samples, validate on 3389 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-84fbb60c9b42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mID_of_layer_to_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ID_of_layer_to_repeat:\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mID_of_layer_to_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_hyperparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maddress_to_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maddress_to_read\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mID_of_layer_to_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mID_of_layer_to_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-8538cafe6f2c>\u001b[0m in \u001b[0;36mselect_hyperparameters\u001b[1;34m(address_to_read, ID_of_layer_to_repeat)\u001b[0m\n\u001b[0;32m     12\u001b[0m                                                                                                 \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                                                                                 \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                                                                                 ID_of_layer_to_repeat = ID_of_layer_to_repeat )\n\u001b[0m\u001b[0;32m     15\u001b[0m         results.append([delta, \n\u001b[0;32m     16\u001b[0m                         \u001b[0mnum_of_train_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-1aede04524a4>\u001b[0m in \u001b[0;36mcreate_model_and_apply_on_data\u001b[1;34m(x_train, y_train, x_test, y_test, max_features, embedding_vector_dim, batch_size, epochs, loss, optimizer, metrics, plot_train_val_graph, number_of_lstm_layers, ID_of_layer_to_repeat)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     50\u001b[0m   score, acc = model.evaluate(x_test, y_test,\n\u001b[0;32m     51\u001b[0m                               batch_size=batch_size)\n",
      "\u001b[1;32mc:\\python37_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\python37_64\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2671\u001b[1;33m                                 session)\n\u001b[0m\u001b[0;32m   2672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2622\u001b[0m         \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2623\u001b[1;33m         \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2624\u001b[0m         \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2625\u001b[0m         \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \"\"\"\n\u001b[0;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1423\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1425\u001b[1;33m               session._session, options_ptr, status)\n\u001b[0m\u001b[0;32m   1426\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#address_to_read= r\"E:/pgmpy\\Twor2009\\Seq of sensor events_based on activities\\based_on_activities.csv\"\n",
    "#address_to_read = r\"E:\\pgmpy\\Twor2009\\Seq of sensor events_no overlap_based on different deltas\\delta_{}min.csv\"\n",
    "address_to_read = r\"E:\\pgmpy\\Twor2009\\Seq of sensor events_based_on_activity_and_no_overlap_delta\\delta_{}min.csv\"\n",
    "print(address_to_read)\n",
    "ID_of_layer_to_repeat = 0\n",
    "print(\"ID_of_layer_to_repeat:\" , ID_of_layer_to_repeat)\n",
    "results = select_hyperparameters(address_to_read = address_to_read , ID_of_layer_to_repeat = ID_of_layer_to_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d20da67344a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(results[:][-1])\n",
    "print(np.argmax(results[:][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (2578, 20)\n",
      "x_test shape: (644, 20)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_104 (Embedding)    (None, None, 64)          7808      \n",
      "_________________________________________________________________\n",
      "gru_69 (GRU)                 (None, 64)                24768     \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 32,641\n",
      "Trainable params: 32,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 2578 samples, validate on 644 samples\n",
      "Epoch 1/5\n",
      "2578/2578 [==============================] - 68s 26ms/step - loss: 0.2978 - acc: 0.8798 - val_loss: 0.8843 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "2578/2578 [==============================] - 5s 2ms/step - loss: 0.1502 - acc: 0.9298 - val_loss: 0.7980 - val_acc: 0.7407\n",
      "Epoch 3/5\n",
      "2578/2578 [==============================] - 5s 2ms/step - loss: 0.1279 - acc: 0.9422 - val_loss: 0.8525 - val_acc: 0.7516\n",
      "Epoch 4/5\n",
      "2578/2578 [==============================] - 5s 2ms/step - loss: 0.1234 - acc: 0.9476 - val_loss: 0.8574 - val_acc: 0.7593\n",
      "Epoch 5/5\n",
      "2578/2578 [==============================] - 6s 2ms/step - loss: 0.1192 - acc: 0.9488 - val_loss: 1.0502 - val_acc: 0.7174\n",
      "644/644 [==============================] - 1s 1ms/step\n",
      "Test score: 1.0502266776487694\n",
      "Test accuracy: 0.717391304347826\n",
      "test_score: 1.0502266776487694\n",
      "test_acc: 0.717391304347826\n",
      "history: {'val_loss': [0.8843139795042714, 0.797959275867628, 0.852543172258768, 0.8573945560810728, 1.0502266776487694], 'val_acc': [0.4953416149068323, 0.7406832298136646, 0.7515527950310559, 0.7593167701863354, 0.717391304347826], 'loss': [0.2977950490658555, 0.1501801671609109, 0.1279152952036347, 0.12335661232841394, 0.1192220518577922], 'acc': [0.8797517456316597, 0.9297905353449222, 0.9422032583397983, 0.947633824716528, 0.9487975174553918]}\n",
      "num_of_train_samples: 2578\n",
      "num_of_test_sample: 644\n"
     ]
    }
   ],
   "source": [
    "address_to_read= r\"E:/pgmpy/Twor2009/Seq of sensor events_based on activities/based_on_activities.csv\"\n",
    "\n",
    "x_train, x_test, y_train, y_test, max_features, maxlen = data_preparation_for_sequences_based_deep_models(address_to_read)\n",
    "\n",
    "test_score, test_acc, history, num_of_train_samples, num_of_test_sample = create_model_and_apply_on_data(x_train, \n",
    "                                                                                                        y_train,\n",
    "                                                                                                        x_test, \n",
    "                                                                                                        y_test, \n",
    "                                                                                                        max_features,\n",
    "                                                                                                        ID_of_layer_to_repeat = ID_of_layer_to_repeat)\n",
    "print(\"test_score:\",test_score)\n",
    "print(\"test_acc:\" , test_acc)\n",
    "print(\"history:\" , history.history)\n",
    "print(\"num_of_train_samples:\" , num_of_train_samples)\n",
    "print(\"num_of_test_sample:\" , num_of_test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bUbyz1O5TtEV",
    "outputId": "fe5bdddd-33a3-4b28-94f0-d6de4758ee37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 10)\n",
      "x_test shape: (25000, 10)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 10)          200010    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                19200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 219,275\n",
      "Trainable params: 219,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 16s 653us/step - loss: 0.6015 - acc: 0.6613 - val_loss: 0.5451 - val_acc: 0.7164\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 13s 529us/step - loss: 0.5163 - acc: 0.7428 - val_loss: 0.5393 - val_acc: 0.7214\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 13s 539us/step - loss: 0.4896 - acc: 0.7652 - val_loss: 0.5306 - val_acc: 0.7294\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 14s 568us/step - loss: 0.4741 - acc: 0.7750 - val_loss: 0.5297 - val_acc: 0.7327\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 14s 557us/step - loss: 0.4628 - acc: 0.7802 - val_loss: 0.5289 - val_acc: 0.7295\n",
      "25000/25000 [==============================] - 2s 65us/step\n",
      "Test score: 0.528882891769\n",
      "Test accuracy: 0.72952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.52888289176940917,\n",
       " 0.72951999999999995,\n",
       " <keras.callbacks.History at 0x261d2fd8908>,\n",
       " 25000,\n",
       " 25000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, max_features, maxlen = imdb_lstm_data_preparation(maxlen=10)\n",
    "#my_x_train, my_x_test, my_y_train, my_y_test, my_max_features, my_maxlen = data_preparation_for_sequences_based_deep_models(address_to_read)\n",
    "#x_train, x_test, y_train, y_test, max_features, maxlen = data_preparation_for_sequences_based_deep_models(address_to_read)#imdb_lstm_data_preparation(maxlen=10)\n",
    "create_model_and_apply_on_data(x_train, y_train,x_test, y_test, max_features,embedding_vector_dim = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rhnw6Y4kQbIz"
   },
   "outputs": [],
   "source": [
    "embeddings = model.layers[0].get_weights()[0]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPT1K2r9RsFo"
   },
   "outputs": [],
   "source": [
    "# `word_to_index` is a mapping (i.e. dict) from words to their index, e.g. `love`: 69\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_to_index.items()}\n",
    "\n",
    "# now you can use it like this for example\n",
    "print(words_embeddings['love'])  # possible output: [0.21, 0.56, ..., 0.65, 0.10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhQ1x75KbI19"
   },
   "outputs": [],
   "source": [
    "print(type(score) , type(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "id": "au02di-ubMqE",
    "outputId": "a68cff59-1a0f-42a6-888b-3ef89722c2ea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6qlruTYcrfl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "imdb_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
